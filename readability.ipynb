{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "from re import sub, findall, match, MULTILINE\n",
    "from csv import reader,writer,QUOTE_ALL\n",
    "from string import punctuation\n",
    "from operator import itemgetter\n",
    "import statistics\n",
    "import scipy.stats as stats\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import FreqDist, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "from textstat.textstat import textstatistics, easy_word_set, legacy_round\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from math import log, sqrt\n",
    "\n",
    "\n",
    "\n",
    "def remove_mention_corpus(text):\n",
    "    new_text = sub(r'@\\w+ ?','',text)\n",
    "    count_matches = len(findall(r'@\\w+ ?',text))\n",
    "    \n",
    "    return new_text, count_matches\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def sentence_level_attr(text, tokens):\n",
    "    word_count, word_sent = 0, 0\n",
    "    words_per_sent = 0.0\n",
    "\n",
    "    #word count of the whole text\n",
    "    vocabulary = FreqDist(tokens)\n",
    "    word_count = len(vocabulary.keys())\n",
    "\n",
    "    #word count by setence\n",
    "    sent_tokenize_list = sent_tokenize(text)\n",
    "    for i in sent_tokenize_list:\n",
    "        tkn = tokenizer(i)\n",
    "        vocabulary = FreqDist(tkn)\n",
    "        word_sent += len(vocabulary.keys())\n",
    "\n",
    "    if(len(sent_tokenize_list) > 0):\n",
    "        words_per_sent = round(word_sent/len(sent_tokenize_list), 2)\n",
    "\n",
    "    return word_count, words_per_sent\n",
    "\n",
    "\n",
    "# Returns Number of Words in the text\n",
    "def word_count(text):\n",
    "    text = sub(r'[^\\w\\s]',' ',text).strip() #remove punct\n",
    "    words = len(tokenizer(text))\n",
    "    return words\n",
    " \n",
    "def character_count(text):\n",
    "    characters = 0 \n",
    "    for token in tokenizer(text):\n",
    "        token = sub(r'[^\\w\\s]','',token) #remove punct.\n",
    "        for t in token.split():\n",
    "            if(bool(match(\"^[A-Za-z0-9]*$\", t)) and t != ''): \n",
    "                characters+= len(t)\n",
    "    return characters\n",
    "\n",
    "def letter_count(text):\n",
    "    letters = 0 \n",
    "    for token in tokenizer(text):\n",
    "        token = sub(r'[^\\D]',' ',token).strip() #remove digits\n",
    "        token = sub(r'[^\\w\\s]',' ',token).strip() #remove punct\n",
    "        for t in token.split():\n",
    "            if(bool(match(\"^[A-Za-z0-9]*$\", t)) and t != ''):\n",
    "                letters+= len(t)\n",
    "    return letters\n",
    "\n",
    "# Returns average sentence length\n",
    "def avg_sentence_length(text):\n",
    "    words = word_count(text)\n",
    "    sentences = len(sent_tokenize(text))\n",
    "    if(sentences > 0):\n",
    "        average_sentence_length = float(words / sentences)\n",
    "    else:\n",
    "        average_sentence_length = 0.0\n",
    "    return average_sentence_length\n",
    "\n",
    "def syllables_count(word):\n",
    "    return textstatistics().syllable_count(word)\n",
    " \n",
    "# Returns the average number of syllables per\n",
    "# word in the text\n",
    "def avg_syllables_per_word(text):\n",
    "    syllable = syllables_count(text)\n",
    "    words = word_count(text)\n",
    "    if(float(words) > 0):\n",
    "        ASPW = float(syllable) / float(words)\n",
    "    else:\n",
    "        ASPW = 0.0\n",
    "    return legacy_round(ASPW, 1)\n",
    " \n",
    "# Return total Difficult Words in a text\n",
    "def difficult_words(text):\n",
    " \n",
    "    # Find all words in the text\n",
    "    words = []\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words += [token for token in sentence]\n",
    " \n",
    "    # difficult words are those with syllables >= 2\n",
    "    # easy_word_set is provide by Textstat as \n",
    "    # a list of common words\n",
    "    diff_words_set = set()\n",
    "     \n",
    "    for word in words:\n",
    "        syllable_count = syllables_count(word)\n",
    "        if word not in easy_word_set and syllable_count >= 2:\n",
    "            diff_words_set.add(word)\n",
    " \n",
    "    return len(diff_words_set)\n",
    "\n",
    "# A word is polysyllablic if it has more than 3 syllables\n",
    "# this functions returns the number of all such words \n",
    "# present in the text\n",
    "def poly_syllable_count(text):\n",
    "    count = 0\n",
    "    words = []\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words += [token for token in sentence]\n",
    "     \n",
    " \n",
    "    for word in words:\n",
    "        syllable_count = syllables_count(word)\n",
    "        if syllable_count >= 3:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def dale_chall_index(text):\n",
    "    \"\"\"\n",
    "        Implements Dale Challe Formula:\n",
    "        Raw score = 0.1579*(PDW) + 0.0496*(ASL) + 3.6365\n",
    "        Here,\n",
    "            PDW = Percentage of difficult words.\n",
    "            ASL = Average sentence length\n",
    "    \"\"\"\n",
    "    words = word_count(text)\n",
    "    # Number of words not termed as difficult words\n",
    "    count = words - difficult_words(text)\n",
    "    if words > 0:\n",
    "        # Percentage of words not on difficult word list\n",
    "        per = float(count) / float(words) * 100\n",
    "    else:\n",
    "        per = 0.0\n",
    "\n",
    "        \n",
    "    # diff_words stores percentage of difficult words\n",
    "    diff_words = 100 - per\n",
    "    raw_score = (0.1579 * diff_words) + \\\n",
    "                (0.0496 * avg_sentence_length(text))\n",
    "     \n",
    "    # If Percentage of Difficult Words is greater than 5 %, then;\n",
    "    # Adjusted Score = Raw Score + 3.6365,\n",
    "    # otherwise Adjusted Score = Raw Score\n",
    " \n",
    "    if diff_words > 5:       \n",
    "        raw_score += 3.6365\n",
    "         \n",
    "    return legacy_round(raw_score, 2)\n",
    "\n",
    "def gunning_fog_index(text):\n",
    "    if(word_count(text) > 0):\n",
    "        per_diff_words = (difficult_words(text) / word_count(text) * 100) + 5\n",
    "    else:\n",
    "        per_diff_words = 5\n",
    "    score = 0.4 * (avg_sentence_length(text) + per_diff_words)\n",
    "    return score\n",
    " \n",
    "def smog_index(text):\n",
    "    \"\"\"\n",
    "        Implements SMOG Formula / Grading\n",
    "        SMOG grading = 3 + ?polysyllable count.\n",
    "        Here, \n",
    "           polysyllable count = number of words of more\n",
    "          than two syllables in a sample of 30 sentences.\n",
    "    \"\"\"\n",
    "    sentence_count = len(sent_tokenize(text))\n",
    "    if sentence_count >= 3:\n",
    "        poly_syllab = poly_syllable_count(text)\n",
    "        SMOG = (1.043 * (30*(poly_syllab / sentence_count))**0.5) \\\n",
    "                + 3.1291\n",
    "        return legacy_round(SMOG, 1)\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def flesch_index(text):\n",
    "    \"\"\"\n",
    "        Implements Flesch Formula:\n",
    "        Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW)\n",
    "        Here,\n",
    "          ASL = average sentence length (number of words \n",
    "                divided by number of sentences)\n",
    "          ASW = average word length in syllables (number of syllables \n",
    "                divided by number of words)\n",
    "    \"\"\"\n",
    "    score = 206.835 - float(1.015 * avg_sentence_length(text)) -\\\n",
    "          float(84.6 * avg_syllables_per_word(text))\n",
    "    \n",
    "    return legacy_round(score, 2) \n",
    "\n",
    "\n",
    "def automated_readby_index(text):\n",
    "    \"\"\"\n",
    "        Automated Readability Index:\n",
    "          ARI = 4.71×(characters/words) + 0.5×(words/sentences) - 21.43\n",
    "        Here,\n",
    "          characters =  number of letters and numbers  \n",
    "    \"\"\"\n",
    "    part1,part2 = 0.0 , 0.0\n",
    "    if(word_count(text)>0):\n",
    "        part1  = (character_count(text)/word_count(text))\n",
    "    else:\n",
    "        part1  = 0.0\n",
    "    \n",
    "    if(len(sent_tokenize(text))>0):        \n",
    "        part2 = (word_count(text)/len(sent_tokenize(text)))\n",
    "    else:\n",
    "        part2 = 0.0\n",
    "\n",
    "    score = 4.71 * part1 + 0.5 * part2 - 21.43\n",
    "\n",
    "    return score\n",
    "\n",
    "def coleman_liau_index(text):\n",
    "    \"\"\"\n",
    "        The Coleman–Liau Index:\n",
    "          CLI = 0.0588×L -0.296×S - 15.8\n",
    "        Here,\n",
    "          L = Letters ÷ Words × 100 (average number of letters per 100 words)\n",
    "          S = Sentences ÷ Words × 100 (average number of sentences per 100 words)\n",
    "    \"\"\"\n",
    "    if(word_count(text) > 0):\n",
    "        L = character_count(text) / word_count(text) * 100\n",
    "        S = len(sent_tokenize(text))/ word_count(text) * 100\n",
    "    else:\n",
    "        L, S = 0.0, 0.0       \n",
    "    score = 0.0588 * L - 0.296 * S - 15.8\n",
    "    return score\n",
    "\n",
    "\n",
    "def read_dataset(filename, label, text_field):\n",
    "    list_docs = []\n",
    "\n",
    "    with open(filename, \"r\") as arq_in:\n",
    "        reader_in = reader(arq_in, delimiter=',', quoting=QUOTE_ALL)\n",
    "        for t in reader_in:\n",
    "            if(len(t[text_field]) > 1):#tratando alguns missing data\n",
    "                list_docs.append([t[text_field], int(t[9]), int(t[10]), int(t[11]),int(t[17])])\n",
    "\n",
    "    return list_docs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textstat.textstat import textstat\n",
    "\n",
    "def characters_stat(text):\n",
    "    try:\n",
    "        return character_count(text)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "def complexWords_stat(text):\n",
    "    try:\n",
    "        return textstat.dale_chall_readability_score(text)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "def longWords_stat(text):\n",
    "    try:\n",
    "        return avg_sentence_length(text)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "def numberSyllables_stat(text):\n",
    "    try:\n",
    "        return textstat.syllable_count(text, lang='en_US')\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "def lexicon_count_stat(text):\n",
    "    try:\n",
    "        return textstat.lexicon_count(text, removepunct=True)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "def sentence_count_stat(text):\n",
    "    try:\n",
    "        return textstat.sentence_count(text)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def flesch_reading_ease_stat(text):\n",
    "    try:\n",
    "        return textstat.flesch_reading_ease(text)\n",
    "    except ValueError:\n",
    "        pass\n",
    "        return 0\n",
    "    \n",
    "def smog_index_stat(text):\n",
    "    try:\n",
    "        return textstat.smog_index(text)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "def flesch_kincaid_grade_stat(text):\n",
    "    try:\n",
    "        return textstat.flesch_kincaid_grade(text)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "def coleman_liau_index_stat(text):\n",
    "    try:\n",
    "        return textstat.coleman_liau_index(text)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "def automated_readability_index_stat(text):\n",
    "    try:\n",
    "        return textstat.automated_readability_index(text)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "def difficult_words_stat(text):\n",
    "    try:\n",
    "        return textstat.difficult_words(text)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "def linsear_write_formula_stat(text):\n",
    "    try:\n",
    "        return textstat.linsear_write_formula(text)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "def gunning_fog_stat(text):\n",
    "    try:\n",
    "        return textstat.gunning_fog(text)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gunning_fog_index(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.79"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flesch_index(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smog_index(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.875999999999998"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coleman_liau_index(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.707000000000001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automated_readby_index(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.71"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dale_chall_index(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_count(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
