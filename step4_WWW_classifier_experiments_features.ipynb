{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "from re import sub, findall, MULTILINE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from unicodedata import normalize\n",
    "from json import loads\n",
    "from csv import reader,writer,QUOTE_ALL\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "\n",
    "from pickle import load\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#IMPORTING EXTERNAL JUPYTER NOTEBOOKS\n",
    "# import nbimporter\n",
    "# from readability import *\n",
    "# import nltk_linguistic_features as nltk_linguis_features\n",
    "# import utils as utils\n",
    "# from importlib import reload\n",
    "# import sys\n",
    "# reload(sys.modules['readability'])\n",
    "# reload(sys.modules['nltk_linguistic_features'])\n",
    "# reload(sys.modules['utils'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######--------------------DICTIONARY OF FEATURES--------------------#######\n",
    "# Summary:\n",
    "# h_basic_filter = [0:3]\n",
    "# h_webmarkup_filter = [4:17]\n",
    "# h_liwc_filter = [18:110]\n",
    "# h_readability_filter = [111:129]\n",
    "# h_nlkt_filter = [130:162]\n",
    "\n",
    "#Details\n",
    "# features = {0:'urls', 1:'headline', 2:'content or headline+content', 3:'date', \n",
    "            \n",
    "#             4:'author', 5:'freq_basic', 6:'freq_formatting', 7:'freq_forms_inputs', 8:'freq_frames', 9:'freq_images', \n",
    "#             10:'freq_audio_video', 11:'freq_links', 12:'freq_lists', 13:'freq_tables', 14:'freq_styles_semantics', \n",
    "#             15:'freq_meta_info', 16:'freq_programming_info', 17:'freq_ads', \n",
    "            \n",
    "#             18:'WC', 19:'Analytic', 20:'Clout', 21:'Authentic', 22:'Tone', 23:'WPS', 24:'Sixltr', 25:'Dic', \n",
    "#             26:'function', 27:'pronoun', 28:'ppron', 29:'i', 30:'we', 31:'you', 32:'shehe', 33:'they', 34:'ipron', \n",
    "#             35:'article', 36:'prep', 37:'auxverb', 38:'adverb', 39:'conj', 40:'negate', 41:'verb', 42:'adj', \n",
    "#             43:'compare', 44:'interrog', 45:'number', 46:'quant', 47:'affect', 48:'posemo', 49:'negemo', 50:'anx', \n",
    "#             51:'anger', 52:'sad', 53:'social', 54:'family', 55:'friend', 56:'female', 57:'male', 58:'cogproc', \n",
    "#             59:'insight', 60:'cause', 61:'discrep', 62:'tentat', 63:'certain', 64:'differ', 65:'percept', 66:'see', \n",
    "#             67:'hear', 68:'feel', 69:'bio', 70:'body', 71:'health', 72:'sexual', 73:'ingest', 74:'drives', \n",
    "#             75:'affiliation', 76:'achieve', 77:'power', 78:'reward', 79:'risk', 80:'focuspast', 81:'focuspresent', \n",
    "#             82:'focusfuture', 83:'relativ', 84:'motion', 85:'space', 86:'time', 87:'work', 88:'leisure', 89:'home', \n",
    "#             90:'money', 91:'relig', 92:'death', 93:'informal', 94:'swear', 95:'netspeak', 96:'assent', 97:'nonflu', \n",
    "#             98:'filler', 99:'AllPunc', 100:'Period', 101:'Comma', 102:'Colon', 103:'SemiC', 104:'QMark', 105:'Exclam', \n",
    "#             106:'Dash', 107:'Quote', 108:'Apostro', 109:'Parenth', 110:'OtherP', \n",
    "            \n",
    "#             111:'characters', 112:'complexWords', 113:'longWords', 114:'numberSyllables', 115:'lexicon_count', \n",
    "#             116:'sentence_count', 117:'flesch_reading_ease', 118:'smog_index', 119:'flesch_kincaid_grade', \n",
    "#             120:'coleman_liau_index', 121:'automated_readability_index', 122:'difficult_words', \n",
    "#             123:'linsear_write_formula', 124:'gunning_fog', 125:'word_count', 126:'words_per_sent', \n",
    "#             127:'nr_captalized_words', 128:'nr_per_stopwords', 129:'count_urls', \n",
    "            \n",
    "#             130:'conjunction, coordinating', 131:'numeral, cardinal', 132:'determiner', 133:'foreign word', \n",
    "#             134:'preposition or conjunction, subordinating', 135:'adjective or numeral, ordinal', \n",
    "#             136:'adjective, comparative', 137:'adjective, superlative', 138:'modal auxiliary', \n",
    "#             139:'noun, common, singular or mass', 140:'noun, proper, singular', 141:'noun, proper, plural', \n",
    "#             142:'noun, common, plural', 143:'pre-determiner', 144:'genitive marker', 145:'pronoun, personal', \n",
    "#             146:'pronoun, possessive', 147:'adverb', 148:'adverb, comparative', 149:'adverb, superlative', \n",
    "#             150:'particle', 151:'\"to\" as preposition/infinitive', 152:'interjection', 153:'verb, base form', \n",
    "#             154:'verb, past tense', 155:'verb, present participle or gerund', 156:'verb, past participle', \n",
    "#             157:'verb, present tense, not 3rd person singular', 158:'verb, present tense, 3rd person singular', \n",
    "#             159:'WH-determiner', 160:'WH-pronoun', 161:'WH-pronoun, possessive', 162:'Wh-adverb'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_features_by_index(features, filter_indexes):\n",
    "    features_temp = []\n",
    "    for i in range(len(features)):\n",
    "        if i not in filter_indexes:\n",
    "            features_temp.append(features[i])\n",
    "    features_filtered = features_temp\n",
    "    return features_filtered\n",
    "\n",
    "# features = [4,5,7,17,22,25,26,29,32,33] #17, 22, 26\n",
    "# removee = [3,4, 6]\n",
    "# arraydd = filter_features(features, removee)\n",
    "# print(arraydd)\n",
    "# print(filter_features(features, removee))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features (Web-markup, LIWIC, Readability and NLTK features)\n",
    "\n",
    " <b>Function: </b> extract_features_filter(typefeatures, type_text, filename, label, filter_features). This function selects some of the features: Web-markup, LIWIC, Readability and NLTK features. A previous process was done to identify the relevant features (see the published paper). Here we are just getting these preselected features. This fuction do not need to process any features, they SHOULD be already in the csv file. It was done to get the results more quickly. <br />\n",
    " <br />\n",
    " <b>Parameters:</b> <br />\n",
    " <b>typefeatures</b>: It is an array of strings. The values in the array can be: 'linguisticF_NLTK' for NLTK features, 'linguisticF_LIWC' for LIWIC features, 'readabilityF' for readability features, 'webmarkupF' for web-markup features. All the values that are included in the array will be used as features to build the model. <br />\n",
    " <br />\n",
    " <b>type_text:</b> It defines the column index - in the csv file - of the text (headline, content or headline+content) that was used to extract LIWIC, readability and NLTK features. Here, this parameter is used just to filter those news with non-text. This parameter can take the values: 1 or 2. 1 = Headline, 2 = Content, 2 = HeadlineContent. Content and HeadlineContent have the same value (2) because they are saved in the same position (third column) in the csv file. This column can contains the content text or headline+content text (string concatenation). \n",
    " <br />         \n",
    " <b>filename</b>: csv file. This csv file should contains already the following features: urls(1), headline(1), content or headline+content(1), date(1), Webmarkup (14 features), LIWC (93 features), Readability (19 features) and NLKT (33 features).\n",
    " <br />         \n",
    " <b>label:</b>It is a numerical value. it could be 1 or 0. 1 means fake news and 0 means real news.\n",
    " <br />         \n",
    " <b>filter_features:</b> It is an array of integers. The array contains the feature's indexes of the ones that will be filtered. Those features were selected after some additional feature engeeniering      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_filter(typefeatures, type_text, filename, label, filter_features ):\n",
    "    list_docs, list_labels, list_urls = [], [], []\n",
    "    count_urls, count_mentions = 0, 0\n",
    "#     print(filter_features)\n",
    "    filter_liwc = filter_features[0]\n",
    "    filter_readability = filter_features[1]\n",
    "    filter_nltk = filter_features[2]\n",
    "    \n",
    "    #print (\"READY\")\n",
    "    with open(filename, \"r\") as arq_in:\n",
    "        reader_in = reader(arq_in, delimiter=',', quoting=QUOTE_ALL)\n",
    "        for t in reader_in:\n",
    "            \n",
    "            if(len(t[0]) > 1 and len(t[type_text]) > 1):#handling missing features\n",
    "\n",
    "                #get html features\n",
    "                author = t[4]\n",
    "                freq_images, freq_audio_video, freq_links, freq_ads  = 0,0,0,0\n",
    "                if(len(t[9]) > 1):\n",
    "                    freq_images = t[9]\n",
    "                if(len(t[10]) > 1):\n",
    "                    freq_audio_video = t[10]\n",
    "                if(len(t[11]) > 1):\n",
    "                    freq_links = t[11]\n",
    "                if(len(t[17]) > 1):\n",
    "                    freq_ads = t[17]\n",
    "                    \n",
    "                if(len(t[12]) > 1):\n",
    "                    freq_lists = t[12]\n",
    "                if(len(t[14]) > 1):\n",
    "                    freq_styles_semantics = t[14]\n",
    "                if(len(t[5]) > 1):\n",
    "                    freq_basic = t[5]\n",
    "                    \n",
    "                #Adding all features   \n",
    "                item = []\n",
    "                if ('linguisticF_NLTK' in typefeatures):\n",
    "                    linguistic_nltk = t[130:163]\n",
    "                    linguistic_nltk_filtered = filter_features_by_index(linguistic_nltk, filter_nltk)\n",
    "                    item = item + list(linguistic_nltk_filtered) \n",
    "                    \n",
    "                if ('linguisticF_LIWC' in typefeatures):\n",
    "                    linguistic_liwc = t[18:111]\n",
    "                    linguistic_liwc_filtered = filter_features_by_index(linguistic_liwc, filter_liwc)\n",
    "                    item = item + list(linguistic_liwc_filtered)\n",
    "\n",
    "                if ('readabilityF' in typefeatures):\n",
    "                    readability_f = t[111:130]\n",
    "                    readability_f_filtered = filter_features_by_index(readability_f, filter_readability)\n",
    "                    item = item + list(readability_f_filtered)\n",
    "                               \n",
    "                if ('webmarkupF' in typefeatures):\n",
    "                    item = item + [author, freq_images, freq_links, freq_styles_semantics, freq_basic, freq_ads]\n",
    "                    \n",
    "                list_docs.append(item)\n",
    "                list_labels.append(label)\n",
    "                list_urls.append(t[0])\n",
    "    #print (\"Finish\")\n",
    "    arq_in.close()\n",
    "    return list_docs,list_labels, list_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This takes into account the new Web-markup features (it was not used in the WWW paper)\n",
    "#Future work (Zhohan)\n",
    "def extract_features_filter_newWM(typefeatures, type_text, filename, label, filter_features):\n",
    "    list_docs, list_labels, list_urls = [], [], []\n",
    "    count_urls, count_mentions = 0, 0\n",
    "    filter_liwc = filter_features[0]\n",
    "    filter_readability = filter_features[1]\n",
    "    filter_nltk = filter_features[2]\n",
    "    \n",
    "    #print (\"READY\")\n",
    "    with open(filename, \"r\") as arq_in:\n",
    "        reader_in = reader(arq_in, delimiter=',', quoting=QUOTE_ALL)\n",
    "        for t in reader_in:\n",
    "            if ('webmarkupF_new' in typefeatures):\n",
    "                # return all webmarkup features (t[0] is url), t[-19] is div\n",
    "                item = t[1:]\n",
    "                list_docs.append(item)\n",
    "                list_labels.append(label)\n",
    "                list_urls.append(t[0])\n",
    "                continue\n",
    "            \n",
    "            if(len(t[0]) > 1 and len(t[type_text]) > 1):#handling missing features\n",
    "\n",
    "                #get html features\n",
    "                author = t[4]\n",
    "                freq_images, freq_audio_video, freq_links, freq_ads  = 0,0,0,0\n",
    "                if(len(t[9]) > 1):\n",
    "                    freq_images = t[9]\n",
    "                if(len(t[10]) > 1):\n",
    "                    freq_audio_video = t[10]\n",
    "                if(len(t[11]) > 1):\n",
    "                    freq_links = t[11]\n",
    "                if(len(t[17]) > 1):\n",
    "                    freq_ads = t[17]\n",
    "                    \n",
    "                if(len(t[12]) > 1):\n",
    "                    freq_lists = t[12]\n",
    "                if(len(t[14]) > 1):\n",
    "                    freq_styles_semantics = t[14]\n",
    "                if(len(t[5]) > 1):\n",
    "                    freq_basic = t[5]\n",
    "                    \n",
    "                #Adding all features   \n",
    "                item = []\n",
    "                if ('linguisticF_NLTK' in typefeatures):\n",
    "\n",
    "                    linguistic_nltk = t[130:163]\n",
    "                    linguistic_nltk_filtered = filter_features_by_index(linguistic_nltk, filter_nltk)\n",
    "                    if to_print:\n",
    "                        print('has linguisticF_NLTK')\n",
    "                        pprint(linguistic_nltk)\n",
    "                        print(linguistic_nltk_filtered)\n",
    "                    item = item + list(linguistic_nltk_filtered) \n",
    "                    \n",
    "                if ('linguisticF_LIWC' in typefeatures):\n",
    "#                     print('has linguisticF_LIWC')\n",
    "                    linguistic_liwc = t[18:111]\n",
    "                    linguistic_liwc_filtered = filter_features_by_index(linguistic_liwc, filter_liwc)\n",
    "                    item = item + list(linguistic_liwc_filtered)\n",
    "\n",
    "                if ('readabilityF' in typefeatures):\n",
    "#                     print('has readabilityF')\n",
    "                    readability_f = t[111:130]\n",
    "                    readability_f_filtered = filter_features_by_index(readability_f, filter_readability)\n",
    "                    if to_print:\n",
    "                        print('has readabilityF')\n",
    "                        pprint(readability_f)\n",
    "                        print(readability_f_filtered)\n",
    "                    item = item + list(readability_f_filtered)\n",
    "                               \n",
    "                if ('webmarkupF' in typefeatures):\n",
    "#                     print('has webmarkupF')\n",
    "#                     print('WARNING: author is not used!!!')\n",
    "#                     item = item + [freq_images, freq_links, freq_styles_semantics, freq_basic, freq_ads]\n",
    "                    item = item + [author, freq_images, freq_links, freq_styles_semantics, freq_basic, freq_ads]\n",
    "    \n",
    "                list_docs.append(item)\n",
    "                list_labels.append(label)\n",
    "                list_urls.append(t[0])\n",
    "    #print (\"Finish\")\n",
    "    arq_in.close()\n",
    "    return list_docs,list_labels, list_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFIER\n",
    "\n",
    "#Applying classifier over the test dataset using invariant features\n",
    "def test_classifier(dir_file_test_fake, dir_file_test_msm, classifier, typefeatures, type_text, filter_features ):\n",
    "    X_fakenews, fakenews_labels_list, fakenews_list_urls = extract_features_filter(typefeatures, type_text, dir_file_test_fake, 1, filter_features ) #1 means fake news. \n",
    "    X_msmnews, msmnews_labels_list, msmnews_list_urls = extract_features_filter(typefeatures, type_text, dir_file_test_msm, 0, filter_features )#0 means true news.\n",
    "    X = np.asarray(X_fakenews+ X_msmnews)\n",
    "    #print (X)\n",
    "    y_true = np.asarray(fakenews_labels_list + msmnews_labels_list) \n",
    "    # y_pred = classifier.predict(X) \n",
    "    return X, y_true\n",
    "\n",
    "#Using CrossValidation\n",
    "def run_classifiers(dir_file_test_fake, dir_file_test_msm, svm_classifier, features, type_text, filter_features ):\n",
    "    metric = [] #accuracy\n",
    "    for i in range(len(features)):\n",
    "        typefeatures = features[i]\n",
    "        name_features = '-'.join(typefeatures)\n",
    "        X, y_target = test_classifier(dir_file_test_fake, dir_file_test_msm, svm_classifier, typefeatures, type_text, filter_features)\n",
    "        scores = cross_val_score(svm_classifier, X, y_target, cv=5)\n",
    "        print(\"Accuracy based on \" +name_features+ \": %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "        #print(scores)\n",
    "        metric.append(scores.mean())\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature that will be filtered. The arrays contain the feature's indexes. Those features were selected after feature engeeniering\n",
    "#HEADLINES\n",
    "h_liwc_filter = [85, 55, 74, 77]\n",
    "h_readability_filter = [7, 18]\n",
    "h_nlkt_filter = [3, 4, 6, 16, 21, 24, 25, 28, 31, 32]\n",
    "headline_filter_features = [h_liwc_filter, h_readability_filter, h_nlkt_filter]\n",
    "\n",
    "#CONTENT\n",
    "c_liwc_filter = [23, 24, 25, 26, 27, 28]\n",
    "c_readability_filter = []\n",
    "c_nlkt_filter = [2, 13, 18, 20, 22]\n",
    "content_filter_features = [c_liwc_filter, c_readability_filter, c_nlkt_filter]\n",
    "\n",
    "#HEAD_CONTENT\n",
    "hc_liwc_filter = [23, 24, 25, 26, 27, 28]\n",
    "hc_readability_filter = []\n",
    "hc_nlkt_filter = [2, 7, 13, 14, 18, 19, 22, 32]\n",
    "headline_content_filter_features = [hc_liwc_filter, hc_readability_filter, hc_nlkt_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_celebrity = \"/Users/.../Fakenews/paper_fn/Experiments/data/celebrity_LIWC/\"\n",
    "\n",
    "fake_files = ['LIWC2015_fakenews_celebrity.csv','LIWC2015_fakenews_celebrity_content.csv','LIWC2015_fakenews_celebrity_headContent.csv']\n",
    "msm_files = ['LIWC2015_realnews_celebrity.csv', 'LIWC2015_realnews_celebrity_content.csv', 'LIWC2015_realnews_celebrity_headContent.csv']\n",
    "\n",
    "data = [fake_files, msm_files]\n",
    "dataLIWC = data\n",
    "    \n",
    "svm_classifier = svm.SVC( C=3)#kernel = 'linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy based on linguisticF_NLTK: 0.59 (+/- 0.03)\n",
      "Accuracy based on linguisticF_LIWC: 0.50 (+/- 0.00)\n",
      "Accuracy based on readabilityF: 0.49 (+/- 0.09)\n",
      "Accuracy based on webmarkupF: 0.68 (+/- 0.09)\n",
      "Accuracy based on linguisticF_NLTK-linguisticF_LIWC: 0.50 (+/- 0.00)\n",
      "Accuracy based on linguisticF_NLTK-readabilityF: 0.53 (+/- 0.14)\n",
      "Accuracy based on linguisticF_NLTK-webmarkupF: 0.73 (+/- 0.05)\n",
      "Accuracy based on linguisticF_LIWC-readabilityF: 0.50 (+/- 0.00)\n",
      "Accuracy based on linguisticF_LIWC-webmarkupF: 0.50 (+/- 0.00)\n",
      "Accuracy based on readabilityF-webmarkupF: 0.57 (+/- 0.08)\n",
      "Accuracy based on linguisticF_NLTK-linguisticF_LIWC-readabilityF: 0.50 (+/- 0.00)\n",
      "Accuracy based on linguisticF_NLTK-linguisticF_LIWC-webmarkupF: 0.50 (+/- 0.00)\n",
      "Accuracy based on linguisticF_NLTK-readabilityF-webmarkupF: 0.62 (+/- 0.09)\n",
      "Accuracy based on linguisticF_LIWC-readabilityF-webmarkupF: 0.50 (+/- 0.00)\n",
      "Accuracy based on linguisticF_NLTK-linguisticF_LIWC-readabilityF-webmarkupF: 0.50 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "# Based on headline\n",
    "dir_file_test_fake =path_celebrity+ dataLIWC[0][0]\n",
    "dir_file_test_msm =path_celebrity+ dataLIWC[1][0]\n",
    "features = [['linguisticF_NLTK'], ['linguisticF_LIWC'], ['readabilityF'], ['webmarkupF'],\n",
    "            ['linguisticF_NLTK', 'linguisticF_LIWC'], ['linguisticF_NLTK', 'readabilityF'], ['linguisticF_NLTK', 'webmarkupF'],\n",
    "            ['linguisticF_LIWC', 'readabilityF'], ['linguisticF_LIWC', 'webmarkupF'],\n",
    "            ['readabilityF', 'webmarkupF'],\n",
    "            ['linguisticF_NLTK', 'linguisticF_LIWC', 'readabilityF'], ['linguisticF_NLTK', 'linguisticF_LIWC', 'webmarkupF'], ['linguisticF_NLTK', 'readabilityF', 'webmarkupF'],\n",
    "            ['linguisticF_LIWC', 'readabilityF', 'webmarkupF'],\n",
    "            ['linguisticF_NLTK', 'linguisticF_LIWC', 'readabilityF', 'webmarkupF']]\n",
    "type_text = 1  #1 = headline\n",
    "#HEADLINES\n",
    "filter_features = headline_filter_features\n",
    "\n",
    "total_gram = 0\n",
    "total_syntax = 0\n",
    "\n",
    "celebrity_accuracy_headline = run_classifiers(dir_file_test_fake, dir_file_test_msm, svm_classifier, features, type_text, filter_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
