{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from readability.ipynb\n",
      "Importing Jupyter notebook from nltk_linguistic_features.ipynb\n",
      "Importing Jupyter notebook from utils.ipynb\n",
      "Importing Jupyter notebook from readability.ipynb\n",
      "Importing Jupyter notebook from nltk_linguistic_features.ipynb\n",
      "Importing Jupyter notebook from utils.ipynb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'utils.ipynb'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "from re import sub, findall, MULTILINE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from unicodedata import normalize\n",
    "from json import loads\n",
    "from csv import reader,writer,QUOTE_ALL\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "\n",
    "from pickle import load\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#IMPORTING EXTERNAL JUPYTER NOTEBOOKS\n",
    "import nbimporter\n",
    "from readability import *\n",
    "import nltk_linguistic_features as nltk_linguis_features\n",
    "import utils as utils\n",
    "from importlib import reload\n",
    "import sys\n",
    "reload(sys.modules['readability'])\n",
    "reload(sys.modules['nltk_linguistic_features'])\n",
    "reload(sys.modules['utils'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability features\n",
    "- 111: 'characters', \n",
    "- 112: 'complexWords', \n",
    "- 113: 'longWords', \n",
    "- 114: 'numberSyllables', \n",
    "- 115: 'lexicon_count', \n",
    "- 116: 'sentence_count', \n",
    "- 117: 'flesch_reading_ease', \n",
    "- 118: 'smog_index', \n",
    "- 119: 'flesch_kincaid_grade', \n",
    "- 120: 'coleman_liau_index', \n",
    "- 121: 'automated_readability_index', \n",
    "- 122: 'difficult_words', \n",
    "- 123: 'linsear_write_formula', \n",
    "- 124: 'gunning_fog', \n",
    "- 125: 'word_count', \n",
    "- 126: 'words_per_sent', \n",
    "- 127: 'nr_captalized_words', \n",
    "- 128: 'nr_per_stopwords', \n",
    "- 129: 'count_urls', "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK features\n",
    "- 130:'conjunction, coordinating', \n",
    "- 131:'numeral, cardinal', \n",
    "- 132:'determiner', \n",
    "- 133:'foreign word', \n",
    "- 134:'preposition or conjunction, subordinating', \n",
    "- 135:'adjective or numeral, ordinal', \n",
    "- 136:'adjective, comparative', \n",
    "- 137:'adjective, superlative', \n",
    "- 138:'modal auxiliary', \n",
    "- 139:'noun, common, singular or mass', \n",
    "- 140:'noun, proper, singular', \n",
    "- 141:'noun, proper, plural', \n",
    "- 142:'noun, common, plural', \n",
    "- 143:'pre-determiner', \n",
    "- 144:'genitive marker', \n",
    "- 145:'pronoun, personal', \n",
    "- 146:'pronoun, possessive', \n",
    "- 147:'adverb', \n",
    "- 148:'adverb, comparative', \n",
    "- 149:'adverb, superlative', \n",
    "- 150:'particle', \n",
    "- 151:'\"to\" as preposition/infinitive', \n",
    "- 152:'interjection', \n",
    "- 153:'verb, base form', \n",
    "- 154:'verb, past tense', \n",
    "- 155:'verb, present participle or gerund', \n",
    "- 156:'verb, past participle', \n",
    "- 157:'verb, present tense, not 3rd person singular', \n",
    "- 158:'verb, present tense, 3rd person singular', \n",
    "- 159:'WH-determiner', \n",
    "- 160:'WH-pronoun', \n",
    "- 161:'WH-pronoun, possessive', \n",
    "- 162:'Wh-adverb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### headline and content csv files contain the following features:\n",
    "#url = t[0] A\n",
    "#headline = t[1] B\n",
    "#content = t[2] C\n",
    "#publish_date = t[3] D\n",
    "#author = t[4] E\n",
    "#freq_basic = t[5] F\n",
    "#freq_formatting = t[6] G\n",
    "#freq_forms_inputs = t[7] H\n",
    "#freq_frames = t[8] I\n",
    "#freq_images = t[9] J\n",
    "#freq_audio_video = t[10] K\n",
    "#freq_links = t[11] L \n",
    "#freq_lists = t[12] M\n",
    "#freq_tables = t[13] N\n",
    "#freq_styles_semantics = t[14] O\n",
    "#freq_meta_info = t[15] P\n",
    "#freq_programming_info = t[16] Q\n",
    "#freq_ads = t[17] R\n",
    "\n",
    "#### headlineContent csv file contains the following features (See how change the THIRD column. It will contain the headline + content):\n",
    "#url = t[0] A\n",
    "#headline = t[1] B\n",
    "#headline+content = t[2] C\n",
    "#publish_date = t[3] D\n",
    "#author = t[4] E\n",
    "#freq_basic = t[5] F\n",
    "#freq_formatting = t[6] G\n",
    "#freq_forms_inputs = t[7] H\n",
    "#freq_frames = t[8] I\n",
    "#freq_images = t[9] J\n",
    "#freq_audio_video = t[10] K\n",
    "#freq_links = t[11] L \n",
    "#freq_lists = t[12] M\n",
    "#freq_tables = t[13] N\n",
    "#freq_styles_semantics = t[14] O\n",
    "#freq_meta_info = t[15] P\n",
    "#freq_programming_info = t[16] Q\n",
    "#freq_ads = t[17] R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and save features (Readability and NLTK features)\n",
    "\n",
    " <b>Function: </b> get_save_Readability_NLTK_features(filename, type_text). This function add (rewrite over the same csv file) the following features: Readability (19 features) and NLKT (33 features). <br />\n",
    " <b>Parameters:</b> <br />\n",
    " <b>filename</b>: csv file source. There are three type of csv files: headline, content, or a headlineContent csv file (see above to get more details about these type of csv files). This csv file should contains already the following features: urls(1), headline(1), content or headline+content(1), date(1), Webmarkup (14 features), and LIWC (93 features). LIWIC features are optional but highly recommended.\n",
    " <br />\n",
    " <b>type_text:</b> It defines the type of text (headline, content or headline+content) that will be used to extract readability and NLTK features. 1 = Headline, 2 = Content, 2 = HeadlineContent. Content and HeadlineContent have the same value (2) because <b>it will depend of the type of csv file source</b>. If it is HeadlineContent csv file, so, the third column should contains headline plus content ( string concatenation). But if it is a Content csv file, the third column should contains just the content. FYI: The second column will always contain only the headline in the three types of csv files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_Readability_NLTK_features(filename, type_text):\n",
    "    list_docs, list_labels, list_urls = [], [], []\n",
    "    count_urls, count_mentions = 0, 0\n",
    "    subset = []\n",
    "    index = 1\n",
    "    with open(filename, \"r\") as arq_in:\n",
    "        reader_in = reader(arq_in, delimiter=',', quoting=QUOTE_ALL)\n",
    "        for t in reader_in:\n",
    "            \n",
    "            if(len(t[0]) > 0):#handling missing features\n",
    "                #text, count_mentions = remove_mention_corpus()\n",
    "                \n",
    "                #get features based on NLP\n",
    "                text, count_urls = nltk_linguis_features.remove_ulr_corpus(t[type_text])\n",
    "                tokens = nltk_linguis_features.tokenizer(text)\n",
    "\n",
    "                pos_tag_numbers   = nltk_linguis_features.part_of_speech_tagger(tokens)\n",
    "                word_count, words_per_sent = nltk_linguis_features.sentence_level_attr(text,tokens)\n",
    "                nr_captalized_words  = nltk_linguis_features.count_captalized_words(tokens)\n",
    "                nr_per_stopwords = nltk_linguis_features.count_per_stopwords(tokens)\n",
    "                nr_punctuation = nltk_linguis_features.count_punctuation(text,r'['+punctuation+r']+')\n",
    "                nr_quotes = nltk_linguis_features.count_punctuation(text,r'[\"\\']+')\n",
    " \n",
    "                instance = []\n",
    "    \n",
    "                #Checking if LIWIC features were added.\n",
    "                if( not len(t)>20):\n",
    "                    print('[Warning] LIWIC features were not extracted. Please extract those manually and update the output cvs file. Remenber this is just optional.')\n",
    "                    instance = instance + list(np.zeros(93)) #adding zeros because there is no LIWIC features\n",
    "                    \n",
    "                #Adding READABILITY features\n",
    "                readability = []\n",
    "                if(text.strip()==\"\"):\n",
    "                    readability = list(np.zeros(14))+ [word_count, words_per_sent] +\\\n",
    "                        [nr_captalized_words, nr_per_stopwords]+\\\n",
    "                        [count_urls]\n",
    "                else:\n",
    "                    readability = [characters_stat(text), complexWords_stat(text), longWords_stat(text), numberSyllables_stat(text), lexicon_count_stat(text), sentence_count_stat(text), flesch_reading_ease_stat(text), smog_index_stat(text), flesch_kincaid_grade_stat(text), coleman_liau_index_stat(text), automated_readability_index_stat(text), difficult_words_stat(text), linsear_write_formula_stat(text), gunning_fog_stat(text)] + [word_count, words_per_sent] +\\\n",
    "                        [nr_captalized_words, nr_per_stopwords]+\\\n",
    "                        [count_urls]\n",
    "                instance = instance + readability  \n",
    "                if(len(readability)!=19):\n",
    "                    print(\"ERROR. Missing readability features-------------------\")\n",
    "                    print(len(instance))\n",
    "                \n",
    "                #Adding linguistic features based on NLTK \n",
    "                if(text.strip()==\"\"):\n",
    "                    instance = instance + list(np.zeros(33))\n",
    "                else:\n",
    "                    instance = instance + pos_tag_numbers \n",
    "             \n",
    "                if(len(pos_tag_numbers)!=33):\n",
    "                    print(\"ERROR. Missing NLTK features-------------------\")\n",
    "                    print(len(pos_tag_numbers))\n",
    "                instance = t[0:len(t)] + instance\n",
    "                \n",
    "                #ERROR. Missing some features\n",
    "                if(len(instance)!= 163):\n",
    "                    print('line', index, len(instance))\n",
    "                    print(len(t))\n",
    "                subset.append(instance)\n",
    "                index = index+1\n",
    "    new_namefile_subset = filename #[:-4]+'_includeLIWC'+'.csv'\n",
    "    utils.save_csv_file(subset, new_namefile_subset)\n",
    "    return \"completed\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV files with preprocessed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYU_Crawled_data dataset\n",
    "path_politicalNYUCrawled = \"/Users/.../Fakenews/paper_fn/Experiments/data/NYU_crawled_data_LIWC_Fixed/\"\n",
    "\n",
    "fake_files = ['fakenews_NYUcrawl_headline.csv','fakenews_NYUcrawl_content.csv','fakenews_NYUcrawl_headlineContent.csv']\n",
    "msm_files = ['realnews_NYUcrawl_headline.csv', 'realnews_NYUcrawl_content.csv', 'realnews_NYUcrawl_headlineContent.csv']\n",
    "\n",
    "data = [fake_files, msm_files]\n",
    "dataLIWC = []\n",
    "for i in range(len(data)):\n",
    "    temp = []\n",
    "    for j in range(len(data[i])):\n",
    "        temp.append('LIWC2015_' + data[i][j])\n",
    "    dataLIWC.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------headline-------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'completed'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Based on headline\n",
    "print('------------------------headline-------------------------------')\n",
    "dir_file_test_fake =path_politicalNYUCrawled+ dataLIWC[0][0]\n",
    "dir_file_test_msm =path_politicalNYUCrawled+ dataLIWC[1][0]\n",
    "extract_save_Readability_NLTK_features(dir_file_test_fake, 1) \n",
    "extract_save_Readability_NLTK_features(dir_file_test_msm, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------content-------------------------------\n",
      "-------------------------headline_content------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'completed'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------content-------------------------------')\n",
    "dir_file_test_fake =path_politicalNYUCrawled+ dataLIWC[0][1]\n",
    "dir_file_test_msm =path_politicalNYUCrawled+ dataLIWC[1][1]\n",
    "extract_save_Readability_NLTK_features(dir_file_test_fake, 2) \n",
    "extract_save_Readability_NLTK_features(dir_file_test_msm, 2)# 2 = content \n",
    "print('-------------------------headline_content------------------------------')\n",
    "dir_file_test_fake =path_politicalNYUCrawled+ dataLIWC[0][2]\n",
    "dir_file_test_msm =path_politicalNYUCrawled+ dataLIWC[1][2]\n",
    "extract_save_Readability_NLTK_features(dir_file_test_fake, 2) \n",
    "extract_save_Readability_NLTK_features(dir_file_test_msm, 2)# 2 = content "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
