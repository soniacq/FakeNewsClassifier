{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "from urllib.error import URLError, HTTPError #For python2.7 uses urllib2, and for python3 uses urllib.error \n",
    "from csv import writer,QUOTE_ALL\n",
    "from time import strftime, sleep\n",
    "from operator import itemgetter\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "from feedparser import parse\n",
    "from re import search, sub\n",
    "from requests import get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Get info of comment's feed through RSS'''\n",
    "\n",
    "def get_news_feed(dict_rss):\n",
    "    url_rss = None\n",
    "    feed_list = sorted(dict_rss.items(), key=itemgetter(1), reverse = True)\n",
    "    \n",
    "    if(len(feed_list) > 0):\n",
    "        url_rss = feed_list[0][0]\n",
    "    return url_rss\n",
    "\n",
    "def get_webpage_rss(html_content):\n",
    "\n",
    "    dict_rss = {}\n",
    "    soup_original = BeautifulSoup(html_content, 'html.parser')      \n",
    "    filter_area = soup_original.find_all('head')\n",
    "\n",
    "    if(len(filter_area) > 0):\n",
    "        filter_area = str(filter_area[0])\n",
    "        soup_filter = BeautifulSoup(filter_area, 'html.parser')    \n",
    "    \n",
    "        for link in soup_filter.find_all('link'):\n",
    "            if( (link.get('type') == 'application/rss+xml') and\n",
    "                (link.get('href').find('comment') < 0) and\n",
    "                (link.get('href').find('feed') > 0) ): \n",
    "                    print(link.get('href'))\n",
    "                    dict_rss[link.get('href')] = len(link.get('href'))\n",
    "    url_feed = get_news_feed(dict_rss)\n",
    "    url_info = parse(url_feed)\n",
    "    print(len(url_info.entries))\n",
    "\n",
    "def verify_url_availability(url):\n",
    "    ''' Verify if the URL is still online '''\n",
    "    is_online = True\n",
    "    try:\n",
    "        request_result = get(url, timeout=10)\n",
    "    except HTTPError as e:\n",
    "        print('HTTPError: {}'.format(e.code))\n",
    "        is_online = False\n",
    "    except URLError as e:\n",
    "        print('URLError: {}'.format(e.reason))\n",
    "        is_online = False\n",
    "\n",
    "    return is_online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####### identifying ads ####\n",
    "\n",
    "\n",
    "def check_attributes(tag_list):\n",
    "    count = 0\n",
    "    popular_attr = ['ad', 'ads', 'adv', 'advert', 'advertisement', 'banner']\n",
    "    for t in tag_list:\n",
    "        for attr in popular_attr:\n",
    "            match_id, match_class = None, None\n",
    "            pattern = r'\\s*'+attr+r'[-]'\n",
    "            if(t.get('id') != None):\n",
    "                match_id = search(pattern, r''+t.get('id'))\n",
    "            if(t.get('class') != None):\n",
    "                match_class = search(pattern, r' '.join(t.get('class')))\n",
    "            if(match_id or match_class):\n",
    "                count += 1\n",
    "                break\n",
    "    return count\n",
    "\n",
    "def check_dimensions(tag_list):\n",
    "    #width, height\n",
    "    dimensions = [('300','250'), ('728','90'), ('160', '600'), ('250', '250'), ('240','400'),\n",
    "                 ('336', '280'), ('180', '150'), ('468', '60'), ('234', '60'), ('88', '31'),\n",
    "                 ('120', '90'), ('120', '60'), ('125', '125'), ('120', '240'), ('120', '600'),\n",
    "                 ('300', '600')]\n",
    "    count = 0\n",
    "    for t in tag_list:\n",
    "        for d in dimensions:\n",
    "            if((t.get('width') != None and t.get('height') != None) and\n",
    "               (t.get('width') == d[0] and t.get('height') == d[1])):\n",
    "                count += 1\n",
    "                break\n",
    "    return count\n",
    "\n",
    "def check_descendants_tree(tag_list):\n",
    "    count = 0\n",
    "    for t in tag_list:\n",
    "        for child in t.descendants:\n",
    "            if ((child != None) and (child.name != None)\n",
    "                and child.name.startswith('script')):           \n",
    "                count +=1\n",
    "    return count\n",
    "\n",
    "def count_advertisements(soup):\n",
    "    count = 0\n",
    "    iframe_taglist = soup.find_all('iframe')\n",
    "    div_taglist    = soup.find_all('div')\n",
    "    aside_taglist  = soup.find_all('aside')\n",
    "\n",
    "    #Method 1: Adblock Filter Rules \n",
    "    #To DO\n",
    "\n",
    "    #Method 2: Popular class/id tags' attributes\n",
    "    count += check_attributes(iframe_taglist)\n",
    "    count += check_attributes(div_taglist)    \n",
    "    count += check_attributes(aside_taglist)\n",
    "\n",
    "    #Method 3: Popular Ads dimensions\n",
    "    count += check_dimensions(iframe_taglist)\n",
    "\n",
    "    #Method 4: Check descendants\n",
    "    count += check_descendants_tree(aside_taglist)\n",
    "\n",
    "    return count\n",
    "\n",
    "def count_ads(soup):\n",
    "    #TO DO add others ads companies\n",
    "\n",
    "    count = 0\n",
    "    for t in soup.find_all('script'):\n",
    "        if ((t.get('src') != None) and\n",
    "            (t.get('src')[0].find('google') >= 0)):\t\n",
    "            count = count + 1\n",
    "    \n",
    "    #manual method to indentify ads (checking the html file structure)\n",
    "    more_ads = count_advertisements(soup)\n",
    "    \n",
    "    count = count + more_ads\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_author_name(name):\n",
    "    ''' Clean the html tag content to get the author name '''\n",
    "   \n",
    "    months = r'\\b[jJ]anuary\\b|\\b[fF]ebruary\\b|\\b[mM]arch\\b|\\b[aA]pril\\b|\\b[mM]ay\\b|\\b[jJ]une\\b|\\b[jJ]uly\\b|\\b[aA]ugust\\b|\\b[sS]eptember\\b|\\b[oO]ctober\\b|\\b[nN]ovember\\b|\\b[dD]ecember\\b'\n",
    "    usual_words = r'[pP]osted|[wW]ritten|[pP]ublished'\n",
    "    temp_words = r'\\bam\\b|\\bpm\\b|\\b[oO]n\\b|\\b[iI]n\\b|\\b[aA]t\\b'\n",
    "    punctuations = r'[,.-]'\n",
    "    \n",
    "    author_name = sub(r'\\d+',' ', name)\n",
    "    author_name = sub(punctuations,' ', author_name)  \n",
    "    author_name = sub(months,' ', author_name)\n",
    "    author_name = sub(usual_words,' ', author_name)\n",
    "    author_name = sub(temp_words,' ', author_name)\n",
    "\n",
    "    return author_name.strip()\n",
    "\n",
    "def verify_usual_tags(tag_name, soup):\n",
    "    ''' Verify common html tags which used to have author's name '''\n",
    "    \n",
    "    attributes_list = ['name', 'rel', 'itemprop', 'class', 'id']\n",
    "    values_list = ['author', 'byline', 'dc.creator']\n",
    "    author_name = None\n",
    "    for t in soup.find_all(tag_name):\n",
    "        for attr in attributes_list:\n",
    "            for vals in values_list:\n",
    "                if ((t.get(attr) != None) and\n",
    "                    (t.get(attr)[0].find('comment') < 0) and\n",
    "                    (t.get(attr)[0].find(vals) >= 0)):\n",
    "                    author_name = str(t)\n",
    "                    author_name = sub('<[^<]+?>', ' ', author_name)\n",
    "                    author_name = sub('[<>|:/]|[bB][yY]|[fF]rom','',author_name)\n",
    "    return author_name\n",
    "\n",
    "def verify_usual_words(html_content):\n",
    "    ''' Verify common words which used to have author's name (ex: By/From) '''  \n",
    "    \n",
    "    author_name = None    \n",
    "    pattern = '[bB][yY][\\:\\s]|[fF]rom[\\:\\s]'\n",
    "    match = search(pattern, html_content)\n",
    "    if(match):\n",
    "        pos_ini, pos_fim = match.span()[0], match.span()[1]\n",
    "        line = html_content[pos_ini:pos_fim+100].replace('\\n','')\n",
    "        search_str = sub('<[^<]+?>', ' ', \"<\"+line+\">\")\n",
    "        search_str = sub('[<>|:/]|[bB][yY]|[fF]rom','',search_str)\n",
    "        author_name = search_str\n",
    "    return author_name\n",
    "\n",
    "def get_authors(html_content):\n",
    "    ''' Get new's author name '''\n",
    "    is_author, author = '',0\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')      \n",
    "     \n",
    "    #Method 1: Popular authors tags\n",
    "    popular_authos_tags = ['span','a','p']\n",
    "    for tag in popular_authos_tags:\n",
    "        result = verify_usual_tags(tag, soup)\n",
    "        if(result):\n",
    "            author = clean_author_name(result)\n",
    "            #return author\n",
    "        \n",
    "    #Method 2: Search for by/from in tags content        \n",
    "    result = verify_usual_words(html_content)\n",
    "    if(result):\n",
    "        author = clean_author_name(result)\n",
    "        #return author\n",
    "\n",
    "    if(author != ''):\n",
    "        is_author = 1\n",
    "    else:\n",
    "        is_author = 0  \n",
    "    return is_author\n",
    "\n",
    "def get_url_domain(url):\n",
    "    pos_ini = url.find('://')+3\n",
    "    pos_end = pos_ini+url[pos_ini:].find('/')\n",
    "    url_domain = url[pos_ini:pos_end]\n",
    "    \n",
    "    return url_domain\n",
    "\n",
    "def set_news_attr(attr):\n",
    "    value = ''\n",
    "    if (attr != None):\n",
    "        value = attr\n",
    "    return value\n",
    "\n",
    "def count_tag_occurr(soup, list_tags):\n",
    "    ratio, aux, occurr = 0.0, 0.0, []\n",
    "\n",
    "    for tag in list_tags:\n",
    "        \n",
    "        if(soup.find_all(tag) != None):\n",
    "            occurr.append(len(soup.find_all(tag)))\n",
    "            aux += len(soup.find_all(tag))\n",
    "        else:\n",
    "            occurr.append(0)\n",
    "    \n",
    "    #ratio = aux/total_tags\n",
    "    ratio = aux\n",
    "    return ratio\n",
    "\n",
    "\n",
    "def counting_html_tags(html_content):\n",
    "    \n",
    "    filter_area = BeautifulSoup(html_content, 'html.parser').find_all('body')\n",
    "    \n",
    "    if(len(filter_area) <=0):\n",
    "        return []\n",
    "    \n",
    "    html_body_content = str(filter_area[0])\n",
    "    soup = BeautifulSoup(html_body_content, 'html.parser')\n",
    "\n",
    "    basic = ['title','h1','h2','h3','h4','h5','h6','p','br','hr']\n",
    "\n",
    "    formatting = ['acronym','abbr', 'address','b','bdi','bdo','big','blockquote','center',\n",
    "                 'cite','code','del','dfn','em','font','i','ins','kbd','mark','meter','pre',\n",
    "                 'progress','q','rp','rt','ruby','s','samp','small','strike','strong',\n",
    "                 'sub','sup','template','time','tt','u','var','wbr']\n",
    "\n",
    "    forms_inputs = ['form','input','textarea','button','select','optgroup','option',\n",
    "                       'label','fieldset','legend','datalist','output']\n",
    "\n",
    "    frames = ['frame','frameset','noframes','iframe']\n",
    "\n",
    "    images = ['img','map','area','canvas','figcaption','figure','picture','svg']\n",
    "\n",
    "    audio_video = ['audio','source','track','video']\n",
    "\n",
    "    links = ['a','link','nav']\n",
    "\n",
    "    lists = ['ul','ol','li','dir','dl','dt','dd','menu','menuitem']\n",
    "\n",
    "    tables = ['table','caption','th','tr','td','thead','tbody','tfoot','col','colgroup']\n",
    "\n",
    "    styles_semantics = ['style','div','span','header','footer','main','section','article','aside',\n",
    "                       'details','dialog','summary','data']\n",
    "\n",
    "    meta_info = ['head','meta','base','basefont']\n",
    "\n",
    "    programming = ['script','noscript','applet','embed','object','param']\n",
    "\n",
    "    ratio_basic = count_tag_occurr(soup, basic)\n",
    "    ratio_formatting = count_tag_occurr(soup, formatting )\n",
    "    ratio_forms_inputs = count_tag_occurr(soup, forms_inputs )\n",
    "    ratio_frames = count_tag_occurr(soup, frames )\n",
    "    ratio_images = count_tag_occurr(soup, images )\n",
    "    ratio_audio_video = count_tag_occurr(soup, audio_video )\n",
    "    ratio_links = count_tag_occurr(soup, links )\n",
    "    ratio_lists = count_tag_occurr(soup, lists )\n",
    "    ratio_tables = count_tag_occurr(soup, tables )\n",
    "    ratio_styles_semantics = count_tag_occurr(soup, styles_semantics )\n",
    "    ratio_meta_info = count_tag_occurr(soup, meta_info )\n",
    "    ratio_programming_info = count_tag_occurr(soup, programming )\n",
    "    nro_ads = count_ads(soup)\n",
    "\n",
    "    return [ratio_basic, ratio_formatting, ratio_forms_inputs, ratio_frames,ratio_images, \n",
    "            ratio_audio_video, ratio_links, ratio_lists, ratio_tables, ratio_styles_semantics,\n",
    "            ratio_meta_info,ratio_programming_info, nro_ads]\n",
    "\n",
    "def get_url_info(html, url):\n",
    "    ''' Get new's attributes '''\n",
    "    try:\n",
    "        article = Article('')\n",
    "        article.set_html(u\"\"+html)\n",
    "        article.parse()\n",
    "\n",
    "        url_domain = get_url_domain(url) \n",
    "        headline = set_news_attr(article.title)\n",
    "        author = get_authors(article.html)\n",
    "        content = set_news_attr(article.text)\n",
    "        publish_date = article.publish_date\n",
    "\n",
    "        if (publish_date):\n",
    "            publish_date = publish_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            publish_date = ''\n",
    "        html_count_info = counting_html_tags(article.html)\n",
    "\n",
    "        if(html_count_info != []):\n",
    "            return [url, headline, content, publish_date, author]+html_count_info\n",
    "        else:\n",
    "            return []\n",
    "    except:\n",
    "        print('Empty html!')\n",
    "        return None  \n",
    "    #return None\n",
    "\n",
    "def store_info_urls(urls_info_list, name_csv_file):\n",
    "    arq_out  = open(name_csv_file, \"w\")\n",
    "    writer_out = writer(arq_out, delimiter=',', quoting=QUOTE_ALL)\n",
    "\n",
    "    for url in urls_info_list:\n",
    "        writer_out.writerow(tuple(url))\n",
    "    \n",
    "    arq_out.close()\n",
    " \n",
    "    \n",
    "def get_info_from_url(url):\n",
    "    ''' Get new's attributes '''\n",
    "    try:\n",
    "        print(url)\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.html\n",
    "        #print(article.html)\n",
    "        #article = Article('')\n",
    "        #article.set_html(u\"\"+html)\n",
    "        article.parse()\n",
    "        print ('article--')\n",
    "        #url_domain = get_url_domain(url) \n",
    "        headline = set_news_attr(article.title)\n",
    "        author = get_authors(article.html)\n",
    "        content = set_news_attr(article.text)\n",
    "        publish_date = article.publish_date\n",
    "        if (publish_date):\n",
    "            publish_date = publish_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            publish_date = ''\n",
    "        html_count_info = counting_html_tags(article.html)\n",
    "        if(html_count_info != []):\n",
    "            return [url, headline, content, publish_date, author]+html_count_info\n",
    "        else:\n",
    "            return []\n",
    "    except:\n",
    "        print('Empty html!')\n",
    "        return None  \n",
    "    #return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASIC AND WEB-MARKUP FEATURES\n",
    "\n",
    "#### Basic features:\n",
    "- url = t[0]\n",
    "- headline = t[1] \n",
    "- content = t[2] \n",
    "- publish_date = t[3] \n",
    "- author = t[4] \n",
    "\n",
    "#### Web-markup features:\n",
    "- freq_basic = t[5] \n",
    "- freq_formatting = t[6] \n",
    "- freq_forms_inputs = t[7] \n",
    "- freq_frames = t[8] \n",
    "- freq_images = t[9] \n",
    "- freq_audio_video = t[10] \n",
    "- freq_links = t[11] \n",
    "- freq_lists = t[12] \n",
    "- freq_tables = t[13] \n",
    "- freq_styles_semantics = t[14] \n",
    "- freq_meta_info = t[15] \n",
    "- freq_programming_info = t[16] \n",
    "- freq_ads = t[17] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features (basics and web-markup features) from a WARC file\n",
    " <b>Function: </b> get_WebFeatures_from_WARCfile(file_name, output_file_name) <br />\n",
    " <b>Parameters:</b> <br />\n",
    " <b>file_name:</b> WARC file, <br />\n",
    " <b>output_file_name:</b> csv file where the features will be save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_WebFeatures_from_WARCfile(file_name, output_file_name):\n",
    "    urls_info_list, id_url = [], 1\n",
    "    \n",
    "    with open(file_name, 'rb') as stream:\n",
    "        for record in ArchiveIterator(stream):\n",
    "            print(int(record.length))\n",
    "            if int(record.length) > 0 :\n",
    "                \n",
    "                html_content = record.content_stream().read().decode('utf-8','ignore')\n",
    "                url_target = record.rec_headers['WARC-Target-URI']\n",
    "                #info_news = get_url_info(html_content,url)\n",
    "                print(url_target)\n",
    "                try:\n",
    "                    info_news = get_url_info(html_content,url_target)\n",
    "                    if (info_news):\n",
    "                        urls_info_list.append(info_news)\n",
    "                        id_url+=1\n",
    "\n",
    "                    if(id_url % 500 == 0):\n",
    "                        print(id_url)\n",
    "                except:\n",
    "                    print('Oppa',id_url)\n",
    "    print(\"Writing...\",len(urls_info_list))\n",
    "    store_info_urls(urls_info_list, output_file_name)\n",
    "    \n",
    "    #store_info_urls(urls_info_list, output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_warc_data = '/Users/.../Fakenews/paper_fn/Experiments/data/discovery_paper/'\n",
    "output_file_name = '/Users/.../Fakenews/paper_fn/Experiments/data/discovery_paper/discoveryCrawlData_Test2019.csv'\n",
    "urls_crawled = get_WebFeatures_from_WARCfile(path_warc_data+'crawl_data-20190223091156448-00240.warc.gz', output_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features (basics and web-markup features) from a URL\n",
    " <b>Function: </b> get_WebFeatures_from_url(url_target, output_filename) <br />\n",
    " <b>Parameters:</b> <br />\n",
    "     <b>url_target:</b> url, <br />\n",
    "     <b>output_filename:</b> csv file where the features will be save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download html from a urls\n",
    "def get_WebFeatures_from_url(url_target, output_filename):\n",
    "    urls_info_list = []        \n",
    "    id =0\n",
    "\n",
    "    try:\n",
    "        info_news = get_info_from_url(url_target)\n",
    "        if (info_news):\n",
    "            urls_info_list.append(info_news)\n",
    "            id+=1\n",
    "\n",
    "        if(id % 500 == 0):\n",
    "            print(id)\n",
    "    except:\n",
    "        print('Oppa',id)\n",
    "\n",
    "    print(\"Writing...\",len(urls_info_list))\n",
    "    store_info_urls(urls_info_list, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.celebdirtylaundry.com/2017/brad-pitt-texts-jennifer-aniston-nonstop-seeks-intense-emotional-support-after-angelina-jolie-divorce/\n",
      "article--\n",
      "Writing... 1\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.celebdirtylaundry.com/2017/brad-pitt-texts-jennifer-aniston-nonstop-seeks-intense-emotional-support-after-angelina-jolie-divorce/'\n",
    "output_file_name = '/Users/.../Fakenews/paper_fn/.../test_fake_celebrity.csv'\n",
    "get_WebFeatures_from_url(url, output_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features (basics and web-markup features) from HTML files. A json file MUST be included inside the directory that contains the hmtls files.\n",
    "\n",
    " <b>Function: </b> get_WebFeatures_from_directory_json(dir_data, output_filename) <br />\n",
    " <b>Parameters:</b> <br />\n",
    " <b>dir_data</b>: Directory path that contains the html files as well as the json file. The json file contains an array objects where each object contains html and url attributes. Json file example:  \n",
    " {data: [ {html: 1.html, url: www.notice.com/note1}, ... , {html: 2.html, url: www.notice2.com/note2}] }\n",
    " <br />\n",
    " <b>output_filename:</b> csv file where the features will be save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import chdir, listdir, environ, makedirs, rename, chmod, walk, remove, path\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "\n",
    "#Getting columns (headline, description, [tags_features]) from a directory that contains json's files (with url and html attributes per line)\n",
    "def get_WebFeatures_from_directory_json(dir_data, output_filename):\n",
    "    urls_info_list, id_url = [], 1\n",
    "    for (dirpath, dirnames, filenames) in walk(dir_data):\n",
    "        for json_file in filenames:\n",
    "            path_json = path.join(dirpath, json_file)\n",
    "            filename = path_json\n",
    "            print (json_file)\n",
    "            if json_file.endswith('.json'):\n",
    "                with open(filename) as json_file:\n",
    "#                     try:\n",
    "                    data = json.load(json_file)\n",
    "                    for p in data['data']:\n",
    "                        url, html_path = p['url'], p['html']\n",
    "                        html_content = \"\"\n",
    "                        with open(path.join(dirpath, html_path)) as fin:\n",
    "                            html_content = fin.read()\n",
    "                        info_news = get_url_info(html_content,url)\n",
    "                        if(info_news):\n",
    "                            urls_info_list.append(info_news)\n",
    "                            #print(info_news)\n",
    "                            id_url += 1 \n",
    "                            print (str(id_url) + \", \")\n",
    "                            #break\n",
    "#                     except:\n",
    "#                         print('Empty html!')\n",
    "            #break\n",
    "        #break\n",
    "                #name_file_csv = path_output_directory+json_file[:-4]+'csv' \n",
    "                #print (name_file_csv)\n",
    "    store_info_urls(urls_info_list, output_filename)\n",
    "    print (\"done\") \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '/Users/.../Fakenews/paper_fn/OfficialVersionExperiment/data/celebrity_html_files_v2019/fake_news/'\n",
    "output_filename = dir_data + 'Celebrity_fake_updated2019.csv'\n",
    "get_WebFeatures_from_directory_json(dir_data, output_filename)\n",
    "\n",
    "dir_data = '/Users/.../Fakenews/paper_fn/OfficialVersionExperiment/data/celebrity_html_files_v2019/real_news/'\n",
    "output_filename = dir_data + 'Celebrity_real_updated2019.csv'\n",
    "get_WebFeatures_from_directory_json(dir_data, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
