{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import chdir, listdir, environ, makedirs, rename, chmod, walk, remove, path\n",
    "from csv import reader,writer,QUOTE_ALL\n",
    "import re\n",
    "import codecs\n",
    "from random import sample\n",
    "\n",
    "#generate training data\n",
    "from math import log, sqrt\n",
    "import math\n",
    "\n",
    "#UTILS\n",
    "\n",
    "#Save csv files\n",
    "def save_csv_file(urls_info_list, name_csv_file):\n",
    "    arq_out  = open(name_csv_file, \"w\")\n",
    "    writer_out = writer(arq_out, delimiter=',', quoting=QUOTE_ALL)\n",
    "\n",
    "    for url in urls_info_list:\n",
    "        writer_out.writerow(tuple(url))\n",
    "    \n",
    "    arq_out.close()\n",
    "\n",
    "\n",
    "#Save csv files\n",
    "def create_head_cont_csv(filename):\n",
    "    subset = []\n",
    "    with open(filename, \"r\") as arq_in:\n",
    "        reader_in = reader(arq_in, delimiter=',', quoting=QUOTE_ALL)\n",
    "        for t in reader_in:\n",
    "            instance = []\n",
    "            headCont = t[1]+\" \"+t[2]\n",
    "            instance = [t[0], t[1], headCont] + t[3:len(t)]\n",
    "            subset.append(instance)\n",
    "    new_namefile_subset = filename[:-4]+'_headContent'+'.csv'\n",
    "    utils.save_csv_file(subset, new_namefile_subset)\n",
    "    return \"completed\"\n",
    "\n",
    "#Save csv files\n",
    "def create_subset_csv_file(filename):\n",
    "    subset = []\n",
    "    with open(filename, \"r\") as arq_in:\n",
    "        reader_in = reader(arq_in, delimiter=',', quoting=QUOTE_ALL)\n",
    "        for t in reader_in:\n",
    "            instance = []\n",
    "            headCont = t[1]+\" \"+t[2]\n",
    "            instance = [t[1], t[2], headCont]\n",
    "            subset.append(instance)\n",
    "    new_namefile_subset = filename[:-4]+'_SUBSet'+'.csv'\n",
    "    \n",
    "    save_csv_file(subset, new_namefile_subset)\n",
    "    return \"completed\"\n",
    "    \n",
    "#Getting domain name from url.\n",
    "def get_url_domain(url):\n",
    "    pos_ini = url.find('://')+3\n",
    "    pos_end = pos_ini+url[pos_ini:].find('/')\n",
    "    url_domain = url[pos_ini:pos_end]\n",
    "    if url_domain.startswith('www.'):\n",
    "        url_domain = url_domain[4:]\n",
    "    return url_domain\n",
    "\n",
    "#Generating a dictionary based on unique domains. That dictionary is indexed by domain names (keys). Each domain name is associated with the total number of urls under that domain (value). \n",
    "#input: csv file.\n",
    "#output: dictionary. ['domain_name_1':total_of_urls_domain_1, 'domain_name_2':total_of_urls_domain_2, ...]\n",
    "def getUniqueDomains_urls(filename):\n",
    "    pages_per_domain = {}\n",
    "    with open(filename, \"r\") as arq_in:\n",
    "        reader_in = reader(arq_in, delimiter=',', quoting=QUOTE_ALL)\n",
    "        for t in reader_in:\n",
    "            domain_name = get_url_domain(t[0])\n",
    "            if domain_name=='':\n",
    "                domain_name='EmptyDomain'\n",
    "            if domain_name not in pages_per_domain:\n",
    "                pages_per_domain[domain_name] = 1\n",
    "            else:\n",
    "                pages_per_domain[domain_name] = pages_per_domain[domain_name]+1\n",
    "    return pages_per_domain\n",
    "\n",
    "#Adding missed domain info as a feature\n",
    "def add_domain_info(filename):\n",
    "    pages_per_domain = {}\n",
    "    all_instances = []\n",
    "    with open(filename, \"r\") as arq_in:\n",
    "        reader_in = reader(arq_in, delimiter=',', quoting=QUOTE_ALL)\n",
    "        for t in reader_in:\n",
    "            instance=[]\n",
    "            domain_name = get_url_domain(t[0])\n",
    "            if domain_name=='':\n",
    "                domain_name='EmptyDomain'\n",
    "            if domain_name not in pages_per_domain:\n",
    "                pages_per_domain[domain_name] = 1\n",
    "            else:\n",
    "                pages_per_domain[domain_name] = pages_per_domain[domain_name]+1\n",
    "            instance = [t[0], domain_name]+t[1:len(t)]\n",
    "            all_instances.append(instance)\n",
    "    new_namefile = filename[:-4]+'_complete'+'.csv'         \n",
    "    save_csv_file(all_instances, new_namefile)\n",
    "    return \"completed\"\n",
    "\n",
    "# Present information about the data: Total number of domains per dataset, number of urls per domain, total of urls per dataset\n",
    "def showInfoData(filename):\n",
    "    unique_fakedomains = getUniqueDomains_urls(filename)\n",
    "    print (\"TOTAL OF UNIQUE DOMAINS: \" + str(len(unique_fakedomains)))\n",
    "    count_fn =0 #total of urls\n",
    "    traindata_pos_domain = [] #traindata from fake domains\n",
    "    for domain in unique_fakedomains:\n",
    "        count_fn = count_fn + unique_fakedomains[domain]\n",
    "        print(domain + \",\" + str(unique_fakedomains[domain]))\n",
    "    print (\"TOTAL OF URLS: \" + str(count_fn) + \"\\n\")\n",
    "\n",
    "#Save instances if they below to a domain   \n",
    "def saveTrainTestData(filename, update_filename, train_domains):\n",
    "    pages_per_domain = {}\n",
    "    train_instances = []\n",
    "    test_instances = []\n",
    "    with open(filename, \"r\") as arq_in:\n",
    "        reader_in = reader(arq_in, delimiter=',', quoting=QUOTE_ALL)\n",
    "        for t in reader_in:\n",
    "            instance=[]\n",
    "            domain_name = get_url_domain(t[0])#domain_name = t[1]\n",
    "            instance = t[0:len(t)]\n",
    "            if domain_name in train_domains:\n",
    "                train_instances.append(instance)\n",
    "            else:\n",
    "                test_instances.append(instance)\n",
    "    new_namefile_train = update_filename[:-4]+'_train'+'.csv'\n",
    "    new_namefile_test = update_filename[:-4]+'_test'+'.csv'\n",
    "    \n",
    "    save_csv_file(train_instances, new_namefile_train)\n",
    "    save_csv_file(test_instances, new_namefile_test)\n",
    "    return \"completed\"\n",
    "\n",
    "def get_consecutive_domain(domains, total_train, total_test):\n",
    "    train_domains = domains[total_test:len(domains)]\n",
    "    test_domains = domains[0:total_test]\n",
    "    return train_domains, test_domains\n",
    "def isNotInList(current_index, list_index):\n",
    "    if current_index in list_index:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_random_domain(domains, total_test):\n",
    "    indexes = list(range(len(domains)))\n",
    "    indexes_test = sample(indexes, total_test) #choose random indexes (total_test = number of random indexes)\n",
    "    indexes_train = [x for i,x in enumerate(indexes) if isNotInList(i,indexes_test)]\n",
    "\n",
    "    test_domains = [ domains[i] for i in indexes_test ]\n",
    "    train_domains = [ domains[i] for i in indexes_train ]\n",
    "    return train_domains, test_domains\n",
    "\n",
    "#Split dataset to get training and test dataset for FAKENEWS or MSMNEWS\n",
    "def getTraningData(filename, percent, index_new_data):\n",
    "    unique_fakedomains = getUniqueDomains_urls(filename)\n",
    "    domains = list(unique_fakedomains.keys())\n",
    "    #print(domains)\n",
    "    #print(unique_fakedomains)\n",
    "    print (\"TOTAL OF UNIQUE DOMAINS: \" + str(len(domains)))\n",
    "    count_fn =0 #total of urls\n",
    "    \n",
    "    #fakenews: with 32% of domains we get TOTAL OF URLS: 1863 which represent almost 19.6% almost 20%\n",
    "    #msmnews: # with 19% of domains we get TOTAL OF URLS: 1885 which represent almost 19.8% almost 20%\n",
    "    total_test = math.floor(len(domains)*percent/100) \n",
    "    total_train = len(domains)-total_test\n",
    "    \n",
    "    #old approach: Fixed split\n",
    "    #train_domains_fn = domains[total_test:len(domains)]\n",
    "    #test_domains_fn = domains[0:total_test]\n",
    "    #train_domains_msm = domains[total_test:len(domains)]\n",
    "    #test_domains_msm = domains[0:total_test]\n",
    "    train_domains, test_domains = get_random_domain(domains, total_test)\n",
    "    \n",
    "    count_fn = 0\n",
    "    for domain in test_domains:\n",
    "        count_fn = count_fn + unique_fakedomains[domain]\n",
    "        #print(domain + \",\" + str(unique_fakedomains[domain]))\n",
    "        \n",
    "    update_filename = filename[:-4]+'_'+index_new_data+'.csv'\n",
    "    \n",
    "    saveTrainTestData(filename, update_filename, train_domains)    \n",
    "    print (\"TOTAL OF URLS: \" + str(count_fn)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_data = \"/Users/.../Fakenews/paper_fn/Experiments/data/politicsFN/\"\n",
    "# showInfoData(path_data+'fakenews_politic.csv')\n",
    "# showInfoData(path_data+'realnews_politic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getTraningData(path_data+'fakenews_politic.csv', 27, 'REMOVE1')\n",
    "# getTraningData(path_data+'realnews_politic.csv', 20, 'REMOVE1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
